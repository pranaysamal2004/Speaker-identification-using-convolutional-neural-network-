{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57e871b7-6558-43c9-bb3b-68fa465e7018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from IPython.display import display, Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac919cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "voicefile_names = os.listdir(\"C:/Users/Himagnya/Downloads/16000_pcm_speeches/audio\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17b04544",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisefile_names = os.listdir(\"C:/Users/Himagnya/Downloads/16000_pcm_speeches/noise\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfe9b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = 0.1\n",
    "shuffle_seed = 43\n",
    "sample_rate = 16000\n",
    "scale = 0.5\n",
    "batch_size = 128\n",
    "epochs = 15 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "046683e4-a4dc-4c6a-9a91-8512f8fa32d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_path = \"C:/Users/Himagnya/Downloads/16000_pcm_speeches/noise\"\n",
    "audio_path = \"C:/Users/Himagnya/Downloads/16000_pcm_speeches/audio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6286ae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_paths = []\n",
    "\n",
    "for subdir in os.listdir(noise_path):\n",
    "    subdir_path= Path (noise_path) / subdir\n",
    "    if os.path.isdir (subdir_path):\n",
    "        noise_paths += [\n",
    "            os.path.join(subdir_path, filepath)\n",
    "            for filepath in os.listdir(subdir_path)\n",
    "            if filepath.endswith(\".wav\")\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9926a8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Himagnya\\\\Downloads\\\\16000_pcm_speeches\\\\noise\\\\other\\\\exercise_bike.wav',\n",
       " 'C:\\\\Users\\\\Himagnya\\\\Downloads\\\\16000_pcm_speeches\\\\noise\\\\other\\\\pink_noise.wav',\n",
       " 'C:\\\\Users\\\\Himagnya\\\\Downloads\\\\16000_pcm_speeches\\\\noise\\\\_background_noise_\\\\10convert.com_Audience-Claps_daSG5fwdA7o.wav',\n",
       " 'C:\\\\Users\\\\Himagnya\\\\Downloads\\\\16000_pcm_speeches\\\\noise\\\\_background_noise_\\\\doing_the_dishes.wav',\n",
       " 'C:\\\\Users\\\\Himagnya\\\\Downloads\\\\16000_pcm_speeches\\\\noise\\\\_background_noise_\\\\dude_miaowing.wav',\n",
       " 'C:\\\\Users\\\\Himagnya\\\\Downloads\\\\16000_pcm_speeches\\\\noise\\\\_background_noise_\\\\running_tap.wav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04334abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0847e670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling rate of original audio tf.Tensor(22050, shape=(), dtype=int32)\n",
      "shape 1350648\n",
      "84\n",
      "Sampling rate of original audio tf.Tensor(22050, shape=(), dtype=int32)\n",
      "shape 1323000\n",
      "82\n",
      "Sampling rate of original audio tf.Tensor(44100, shape=(), dtype=int32)\n",
      "shape 718514\n",
      "44\n",
      "Sampling rate of original audio tf.Tensor(22050, shape=(), dtype=int32)\n",
      "shape 2098788\n",
      "131\n",
      "Sampling rate of original audio tf.Tensor(22050, shape=(), dtype=int32)\n",
      "shape 1362816\n",
      "85\n",
      "Sampling rate of original audio tf.Tensor(22050, shape=(), dtype=int32)\n",
      "shape 1348479\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "#os.system (command)\n",
    "def load_noise_sample(path):\n",
    "    sample, sampling_rate = tf.audio.decode_wav(\n",
    "        tf.io.read_file(path), desired_channels=1\n",
    "    )\n",
    "    print(\"Sampling rate of original audio\", sampling_rate)\n",
    "    if sampling_rate >= sample_rate:\n",
    "        print(\"shape\", sample.shape [0])\n",
    "        slices = int(sample.shape [0] / sample_rate)\n",
    "        print (slices)\n",
    "        sample= tf.split(sample[: slices* sample_rate], slices)\n",
    "        return sample\n",
    "    else:\n",
    "        print(\"Sampling rate for\", path, \"is incorrect\")\n",
    "        return None\n",
    "noises = []\n",
    "for path in noise_paths: \n",
    "    sample = load_noise_sample(path)\n",
    "    if sample:\n",
    "        noises.extend (sample)\n",
    "noises = tf.stack(noises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd87f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_and_labels_to_dataset (audio_paths, labels):\n",
    "    path_ds = tf.data. Dataset.from_tensor_slices (audio_paths)\n",
    "    audio_ds = path_ds.map (lambda x: path_to_audio (x))\n",
    "    label_ds = tf.data. Dataset.from_tensor_slices (labels)\n",
    "    return tf.data. Dataset.zip((audio_ds, label_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ec62d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_audio (path):\n",
    "    audio = tf.io.read_file (path)\n",
    "    audio, _ = tf.audio.decode_wav (audio, 1, sample_rate)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f8fbc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise (audio, noises=None, scale=0.5):\n",
    "    if noises is not None:\n",
    "        tf_rnd = tf.random.uniform(\n",
    "            (tf.shape (audio) [0],), 0, noises.shape [0], dtype=tf.int32\n",
    "        )\n",
    "        noise = tf.gather (noises, tf_rnd, axis=0)\n",
    "        \n",
    "        prop= tf.math.reduce_max (audio, axis=1) / tf.math.reduce_max (noise, axis=1)\n",
    "        prop= tf.repeat (tf.expand_dims (prop, axis=1), tf.shape (audio)[1], axis=1)\n",
    "        \n",
    "        audio = audio + noise * prop * scale\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3d2b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_fft (audio):\n",
    "    audio = tf.squeeze (audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex (real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims (fft, axis=-1)\n",
    "    return tf.math.abs (fft[:,: (audio.shape [1] // 2), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cad177db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Benjamin_Netanyau', 'Jens_Stoltenberg', 'Julia_Gillard', 'Magaret_Tarcher', 'Nelson_Mandela']\n",
      "Speaker:  Benjamin_Netanyau\n",
      "Speaker:  Jens_Stoltenberg\n",
      "Speaker:  Julia_Gillard\n",
      "Speaker:  Magaret_Tarcher\n",
      "Speaker:  Nelson_Mandela\n"
     ]
    }
   ],
   "source": [
    "class_names = os.listdir (audio_path)\n",
    "print (class_names,)\n",
    "audio_paths = []\n",
    "labels = []\n",
    "for label, name in enumerate (class_names):\n",
    "    print(\"Speaker: \", (name))\n",
    "    dir_path = Path (audio_path) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join (dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label]* len (speaker_sample_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b5423e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle to generate random data\n",
    "rng = np. random. RandomState (shuffle_seed)\n",
    "rng.shuffle (audio_paths)\n",
    "rng = np. random. RandomState(shuffle_seed)\n",
    "rng.shuffle (labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94161150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation\n",
    "num_val_samples = int (valid_split * len(audio_paths))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "valid_audio_paths = audio_paths [-num_val_samples:]\n",
    "valid_labels = labels [-num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b742971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 6751 files for training.\n",
      "Using 750 files for validation.\n"
     ]
    }
   ],
   "source": [
    "num_val_samples = int(valid_split * len(audio_paths))\n",
    "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "\n",
    "print(\"Using {} files for validation.\".format(num_val_samples))\n",
    "valid_audio_paths = audio_paths[-num_val_samples:]\n",
    "valid_labels = labels[-num_val_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a787b92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets, one for training and the other for validation\n",
    "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "train_ds = train_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle_seed).batch(\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=shuffle_seed).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25645607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the training set\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (add_noise (x, noises, scale=scale), y),\n",
    "    num_parallel_calls = tf.data.experimental.AUTOTUNE,\n",
    ")\n",
    "\n",
    "# Transform audio wave to the frequency domain using 'audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft (x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft (x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0294ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a8f53f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ conv1d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_12       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_4     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4000</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1333</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">170624</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling1… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">43,680,000</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_15 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m512\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv1d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_16 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m49,280\u001b[0m │ activation_10[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv1d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_17 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │     \u001b[38;5;34m49,280\u001b[0m │ activation_11[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_14 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │        \u001b[38;5;34m256\u001b[0m │ input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_4 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ conv1d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ conv1d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ activation_12       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ add_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling1d_4     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4000\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ activation_12[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1333\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling1d_4[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m170624\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ average_pooling1… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │ \u001b[38;5;34m43,680,000\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m32,896\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m645\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,812,869</span> (167.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,812,869\u001b[0m (167.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,812,869</span> (167.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,812,869\u001b[0m (167.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def residual_block(x, filters, conv_num= 3, activation = \"relu\"):\n",
    "    s = keras.layers. Conv1D (filters, 1, padding = \"same\") (x)\n",
    "    \n",
    "    for i in range (conv_num - 1):\n",
    "        x = keras.layers. Conv1D (filters, 3, padding = \"same\") (x)\n",
    "        x = keras.layers. Activation (activation) (x)\n",
    "    x = keras.layers. Conv1D (filters, 3, padding = \"same\") (x)\n",
    "    x = keras.layers.Add() ([x, s])\n",
    "    x = keras.layers. Activation (activation) (x)\n",
    "    return keras.layers. MaxPool1D (pool_size = 2, strides= 2)(x)\n",
    "\n",
    "def build_model (input_shape, num_classes):\n",
    "    inputs = keras. layers. Input (shape = input_shape, name = \"input\")\n",
    "\n",
    "    x = residual_block (inputs, 16, 2)\n",
    "    x = residual_block (inputs, 32, 2)\n",
    "    x = residual_block (inputs, 64, 3)\n",
    "    x = residual_block (inputs, 128, 3)\n",
    "    x = residual_block (inputs, 128, 3)\n",
    "    x = keras.layers. AveragePooling1D (pool_size=3, strides=3)(x)\n",
    "    x = keras.layers. Flatten() (x)\n",
    "    x = keras.layers.Dense (256, activation=\"relu\") (x)\n",
    "    x = keras.layers. Dense (128, activation=\"relu\") (x)\n",
    "    outputs = keras.layers.Dense (num_classes, activation = \"softmax\", name = \"output\") (x)\n",
    "    return keras.models. Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "model = build_model((sample_rate // 2, 1), len(class_names))\n",
    "model.summary()\n",
    "model.compile (optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_save_filename= \"model.h5\"\n",
    "earlystopping_cb = keras.callbacks. EarlyStopping (patience=10, restore_best_weights=True)\n",
    "mdlcheckpoint_cb = keras.callbacks. ModelCheckpoint (\n",
    "    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1d069b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6317d33a",
   "metadata": {},
   "source": [
    "Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e0d8846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.3951 - loss: 75.7275 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 10s/step - accuracy: 0.3984 - loss: 74.8623 - val_accuracy: 0.7747 - val_loss: 0.5503\n",
      "Epoch 2/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.8752 - loss: 0.3411"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 10s/step - accuracy: 0.8757 - loss: 0.3398 - val_accuracy: 0.9267 - val_loss: 0.1893\n",
      "Epoch 3/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9462 - loss: 0.1415 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 10s/step - accuracy: 0.9464 - loss: 0.1413 - val_accuracy: 0.9613 - val_loss: 0.1030\n",
      "Epoch 4/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9754 - loss: 0.0782"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 10s/step - accuracy: 0.9753 - loss: 0.0785 - val_accuracy: 0.9680 - val_loss: 0.0932\n",
      "Epoch 5/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9684 - loss: 0.0913 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m545s\u001b[0m 10s/step - accuracy: 0.9684 - loss: 0.0912 - val_accuracy: 0.9760 - val_loss: 0.0698\n",
      "Epoch 6/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 10s/step - accuracy: 0.9784 - loss: 0.0560 - val_accuracy: 0.9720 - val_loss: 0.0744\n",
      "Epoch 7/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 10s/step - accuracy: 0.9729 - loss: 0.0737 - val_accuracy: 0.9600 - val_loss: 0.1289\n",
      "Epoch 8/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9813 - loss: 0.0538"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 10s/step - accuracy: 0.9813 - loss: 0.0537 - val_accuracy: 0.9800 - val_loss: 0.0806\n",
      "Epoch 9/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 10s/step - accuracy: 0.9783 - loss: 0.0632 - val_accuracy: 0.9747 - val_loss: 0.0855\n",
      "Epoch 10/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 10s/step - accuracy: 0.9874 - loss: 0.0420 - val_accuracy: 0.9800 - val_loss: 0.0600\n",
      "Epoch 11/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 10s/step - accuracy: 0.9890 - loss: 0.0317 - val_accuracy: 0.9800 - val_loss: 0.0623\n",
      "Epoch 12/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 10s/step - accuracy: 0.9842 - loss: 0.0446 - val_accuracy: 0.9587 - val_loss: 0.0899\n",
      "Epoch 13/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 10s/step - accuracy: 0.9806 - loss: 0.0486 - val_accuracy: 0.9747 - val_loss: 0.0710\n",
      "Epoch 14/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9920 - loss: 0.0219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 10s/step - accuracy: 0.9920 - loss: 0.0220 - val_accuracy: 0.9827 - val_loss: 0.0793\n",
      "Epoch 15/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 10s/step - accuracy: 0.9809 - loss: 0.0596 - val_accuracy: 0.9827 - val_loss: 0.0531\n",
      "Epoch 16/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m518s\u001b[0m 10s/step - accuracy: 0.9929 - loss: 0.0272 - val_accuracy: 0.9773 - val_loss: 0.0736\n",
      "Epoch 17/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 10s/step - accuracy: 0.9909 - loss: 0.0215 - val_accuracy: 0.9813 - val_loss: 0.0682\n",
      "Epoch 18/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m515s\u001b[0m 10s/step - accuracy: 0.9907 - loss: 0.0308 - val_accuracy: 0.9747 - val_loss: 0.1080\n",
      "Epoch 19/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10s/step - accuracy: 0.9944 - loss: 0.0183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m520s\u001b[0m 10s/step - accuracy: 0.9944 - loss: 0.0182 - val_accuracy: 0.9880 - val_loss: 0.0503\n",
      "Epoch 20/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 10s/step - accuracy: 0.9953 - loss: 0.0125 - val_accuracy: 0.9840 - val_loss: 0.0591\n",
      "Epoch 21/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 10s/step - accuracy: 0.9941 - loss: 0.0161 - val_accuracy: 0.9867 - val_loss: 0.0504\n",
      "Epoch 22/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 10s/step - accuracy: 0.9940 - loss: 0.0191 - val_accuracy: 0.9827 - val_loss: 0.0840\n",
      "Epoch 23/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 10s/step - accuracy: 0.9881 - loss: 0.0437 - val_accuracy: 0.9813 - val_loss: 0.0692\n",
      "Epoch 24/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 10s/step - accuracy: 0.9923 - loss: 0.0288 - val_accuracy: 0.9747 - val_loss: 0.0927\n",
      "Epoch 25/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 10s/step - accuracy: 0.9905 - loss: 0.0311 - val_accuracy: 0.9813 - val_loss: 0.0653\n",
      "Epoch 26/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 10s/step - accuracy: 0.9763 - loss: 0.1447 - val_accuracy: 0.9133 - val_loss: 0.3055\n",
      "Epoch 27/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m528s\u001b[0m 10s/step - accuracy: 0.9248 - loss: 0.2128 - val_accuracy: 0.9600 - val_loss: 0.1536\n",
      "Epoch 28/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m522s\u001b[0m 10s/step - accuracy: 0.9732 - loss: 0.0755 - val_accuracy: 0.9680 - val_loss: 0.1397\n",
      "Epoch 29/50\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 10s/step - accuracy: 0.9767 - loss: 0.0628 - val_accuracy: 0.9653 - val_loss: 0.1016\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs = epochs,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79a2f937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 360ms/step - accuracy: 0.9893 - loss: 0.0474\n",
      "Accuracy of model:  [0.05026037618517876, 0.9879999756813049]\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy of model: \", model.evaluate(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "efd341a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 375ms/step\n",
      "Speaker:\u001b[92m Magaret_Tarcher\u001b \tPredicted:\u001b[92m Magaret_Tarcher\u001b \n",
      "Welcome\n",
      "The speaker is Magaret_Tarcher\n",
      "Speaker:\u001b[92m Julia_Gillard\u001b \tPredicted:\u001b[92m Julia_Gillard\u001b \n",
      "Welcome\n",
      "The speaker is Julia_Gillard\n",
      "Speaker:\u001b[92m Julia_Gillard\u001b \tPredicted:\u001b[92m Julia_Gillard\u001b \n",
      "Welcome\n",
      "The speaker is Julia_Gillard\n",
      "Speaker:\u001b[92m Julia_Gillard\u001b \tPredicted:\u001b[92m Julia_Gillard\u001b \n",
      "Welcome\n",
      "The speaker is Julia_Gillard\n",
      "Speaker:\u001b[92m Magaret_Tarcher\u001b \tPredicted:\u001b[92m Magaret_Tarcher\u001b \n",
      "Welcome\n",
      "The speaker is Magaret_Tarcher\n",
      "Speaker:\u001b[92m Benjamin_Netanyau\u001b \tPredicted:\u001b[92m Benjamin_Netanyau\u001b \n",
      "Welcome\n",
      "The speaker is Benjamin_Netanyau\n",
      "Speaker:\u001b[92m Nelson_Mandela\u001b \tPredicted:\u001b[92m Nelson_Mandela\u001b \n",
      "Welcome\n",
      "The speaker is Nelson_Mandela\n",
      "Speaker:\u001b[91m Jens_Stoltenberg\u001b \tPredicted:\u001b[91m Benjamin_Netanyau\u001b \n",
      "Sorry\n",
      " Benjamin_Netanyau\n",
      "Speaker:\u001b[92m Nelson_Mandela\u001b \tPredicted:\u001b[92m Nelson_Mandela\u001b \n",
      "Welcome\n",
      "The speaker is Nelson_Mandela\n",
      "Speaker:\u001b[92m Nelson_Mandela\u001b \tPredicted:\u001b[92m Nelson_Mandela\u001b \n",
      "Welcome\n",
      "The speaker is Nelson_Mandela\n"
     ]
    }
   ],
   "source": [
    "SAMPLES_TO_DISPLAY = 10\n",
    "test_ds= paths_and_labels_to_dataset (valid_audio_paths, valid_labels)\n",
    "test_ds = test_ds.shuffle (buffer_size = batch_size * 8, seed=shuffle_seed).batch(\n",
    "    batch_size\n",
    ")\n",
    "\n",
    "test_ds = test_ds.map (lambda x, y: (add_noise (x, noises, scale=scale), y))\n",
    "\n",
    "for audios, labels in test_ds.take (1):\n",
    "    ffts = audio_to_fft (audios)\n",
    "    y_pred = model.predict (ffts)\n",
    "    rnd = np. random. randint(0, batch_size, SAMPLES_TO_DISPLAY)\n",
    "    audios = audios.numpy () [rnd, :, :]\n",
    "    labels = labels.numpy ( ) [rnd]\n",
    "    y_pred = np.argmax (y_pred, axis=-1) [rnd]\n",
    "\n",
    "    for index in range (SAMPLES_TO_DISPLAY):\n",
    "        print( \n",
    "            \"Speaker:\\33{} {}\\33 \\tPredicted:\\33{} {}\\33 \".format(\n",
    "                \"[92m\" if labels[index] == y_pred[index] else \"[91m\",\n",
    "                class_names[labels [index]],\n",
    "                \"[92m\" if labels[index] == y_pred [index] else \"[91m\",\n",
    "                class_names[y_pred [index]],\n",
    "            )\n",
    "        )\n",
    "        if labels [index] ==y_pred [index]:\n",
    "            print(\"Welcome\")\n",
    "        else:\n",
    "            print(\"Sorry\")\n",
    "        print(\"The speaker is\" if labels[index]== y_pred [index] else \"\", class_names [y_pred [index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e2a8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dataset (audio_paths):\n",
    "    path_ds = tf.data. Dataset.from_tensor_slices (audio_paths)\n",
    "    return tf.data. Dataset.zip((path_ds))\n",
    "\n",
    "def predict (path, labels):\n",
    "    test = paths_and_labels_to_dataset(path, labels)\n",
    "    \n",
    "    test = test.shuffle(buffer_size=batch_size* 8, seed=shuffle_seed). batch (\n",
    "        batch_size \n",
    "    )\n",
    "    test = test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    test = test.map (lambda x, y: (add_noise (x, noises, scale=scale), y))\n",
    "    \n",
    "    for audios, labels in test.take (1):\n",
    "        ffts = audio_to_fft (audios)\n",
    "        y_pred = model.predict (ffts)\n",
    "        rnd = np.random.randint(0, 1, 1)\n",
    "        audios= audios.numpy () [rnd, :]\n",
    "        labels = labels.numpy () [rnd]\n",
    "        y_pred = np.argmax(y_pred, axis=-1)[rnd]\n",
    "\n",
    "        for index in range (1):\n",
    "                print (\n",
    "                \"Speaker: \\33{} {}\\33 \\tPredicted: \\33{} {}\\33 \" .format(\n",
    "                \"[92m\",y_pred[index],\n",
    "                    \"[92m\", y_pred[index]\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                print(\"Speaker Predicted: \", class_names [y_pred[index]])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8fbb6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\n",
      "Speaker: \u001b[92m 2\u001b \tPredicted: \u001b[92m 2\u001b \n",
      "Speaker Predicted:  Julia_Gillard\n"
     ]
    }
   ],
   "source": [
    "path = [\"C:/Users/Himagnya/Downloads/16000_pcm_speeches/Julia_Gillard/963.wav\"]\n",
    "labels = [\"unknown\"]\n",
    "\n",
    "predict (path, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
